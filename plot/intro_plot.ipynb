{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import dit_fast_attention\n",
    "import os\n",
    "file_path=os.path.abspath(dit_fast_attention.__file__)\n",
    "dir_path=os.path.dirname(file_path)\n",
    "os.chdir(dir_path)\n",
    "from diffusers import DiTPipeline, DPMSolverMultistepScheduler\n",
    "import argparse\n",
    "import torch\n",
    "from evaluation import test_latencies\n",
    "from dit_fast_attention import transform_model_fast_attention\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import thop\n",
    "from utils import calculate_flops\n",
    "os.environ[\"https_proxy\"]=\"http://10.10.20.100:1089\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "n_calib=8\n",
    "n_steps=20\n",
    "calib_x=torch.randint(0, 1000, (n_calib,),generator=torch.Generator().manual_seed(3)).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw_latencies=test_latencies(pipe, n_steps,calib_x,bs=[1])\n",
    "# print(\"Raw latencies\",raw_latencies)\n",
    "raw_flops=calculate_flops(pipe,calib_x[:1],n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "n_calib=8\n",
    "n_steps=20\n",
    "calib_x=torch.randint(0, 1000, (n_calib,),generator=torch.Generator().manual_seed(3)).to(\"cuda\")\n",
    "threshold=0.9\n",
    "use_cache=True\n",
    "pipe,calib_ssim=transform_model_fast_attention(pipe, n_steps=n_steps, n_calib=n_calib, calib_x=calib_x, threshold=threshold, window_size=[-64,64],use_cache=use_cache,seed=3)\n",
    "\n",
    "latencies=test_latencies(pipe, n_steps,calib_x,bs=[1,8])\n",
    "flops=calculate_flops(pipe,calib_x[:1],n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import Transformer2DModel, PixArtSigmaPipeline\n",
    "transformer = Transformer2DModel.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n",
    "    # \"PixArt-alpha/PixArt-Sigma-XL-2-2K-MS\",\n",
    "    subfolder='transformer', \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "pipe = PixArtSigmaPipeline.from_pretrained(\n",
    "    'PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers',\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "pipe.config._name_or_path=transformer.config._name_or_path\n",
    "n_calib=4\n",
    "n_steps=20\n",
    "# calib_x=torch.randint(0, 1000, (n_calib,),generator=torch.Generator().manual_seed(3)).to(\"cuda\")\n",
    "calib_x=[\"cat\"]*n_calib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops=calculate_flops(pipe,calib_x[:1],n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_latencies=test_latencies(pipe, n_steps,calib_x,bs=[1])\n",
    "print(\"Raw latencies\",raw_latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.925\n",
    "use_cache=True\n",
    "pipe,calib_ssim=transform_model_fast_attention(pipe, n_steps=n_steps, n_calib=n_calib, calib_x=calib_x, threshold=threshold, window_size=[-256,256],use_cache=use_cache,seed=3)\n",
    "# pipe,calib_ssim=transform_model_fast_attention(pipe, n_steps=n_steps, n_calib=n_calib, calib_x=calib_x, threshold=threshold, window_size=[-1024,1024],use_cache=use_cache,seed=3)\n",
    "\n",
    "flops=calculate_flops(pipe,calib_x[:1],n_steps)\n",
    "\n",
    "# latencies=test_latencies(pipe, n_steps,calib_x,bs=[1])\n",
    "# print(\"Latencies\",latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import Transformer2DModel, PixArtSigmaPipeline\n",
    "transformer = Transformer2DModel.from_pretrained(\n",
    "    # \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n",
    "    \"PixArt-alpha/PixArt-Sigma-XL-2-2K-MS\",\n",
    "    subfolder='transformer', \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "pipe = PixArtSigmaPipeline.from_pretrained(\n",
    "    'PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers',\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "pipe.config._name_or_path=transformer.config._name_or_path\n",
    "n_calib=2\n",
    "n_steps=20\n",
    "# calib_x=torch.randint(0, 1000, (n_calib,),generator=torch.Generator().manual_seed(3)).to(\"cuda\")\n",
    "calib_x=[\"cat\"]*n_calib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops=calculate_flops(pipe,calib_x[:1],n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.9\n",
    "use_cache=True\n",
    "# pipe,calib_ssim=transform_model_fast_attention(pipe, n_steps=n_steps, n_calib=n_calib, calib_x=calib_x, threshold=threshold, window_size=[-256,256],use_cache=use_cache,seed=3)\n",
    "pipe,calib_ssim=transform_model_fast_attention(pipe, n_steps=n_steps, n_calib=n_calib, calib_x=calib_x, threshold=threshold, window_size=[-1024,1024],use_cache=use_cache,seed=3)\n",
    "\n",
    "flops=calculate_flops(pipe,calib_x[:1],n_steps)\n",
    "\n",
    "# latencies=test_latencies(pipe, n_steps,calib_x,bs=[1])\n",
    "# print(\"Latencies\",latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors=sns.color_palette(\"Set2\")\n",
    "\n",
    "fig=plt.figure(figsize=(5,4))\n",
    "\n",
    "# 假设有两组数据\n",
    "# group1 1\t0.897481743\t1\t0.762847116\t1\t0.495728137\n",
    "# group2 0.871081781\t0.82601481\t0.673733854\t0.610346823\t0.338653802\t0.302214254\n",
    "\n",
    "# group1 = [1,0.897481743,1,0.836255068,1,0.701581777]\n",
    "# group2 = [0.871081781,0.82601481,0.498965392,0.468643292,0.197997179,0.183244937]\n",
    "group1=[1,0.897481743,1,0.762847116,1,0.495728137]\n",
    "group2=[0.871081781,0.82601481,0.673733854,0.610346823,0.338653802,0.302214254]\n",
    "\n",
    "\n",
    "raw_attn=group1[::2]\n",
    "fast_attn=group1[1::2]\n",
    "raw_other=group2[1::2]\n",
    "\n",
    "\n",
    "# 创建 x 轴标签\n",
    "x = np.arange(len(raw_attn))*1.5\n",
    "labels = ['512x512\\nraw', '512x512\\nDitFastAttn', '1024x1024\\nraw', '1024x1024\\nDitFastAttn', '2048x2048\\nraw', '2048x2048\\nDitFastAttn']\n",
    "labels = ['512x512', '1024x1024', '2048x2048']\n",
    "\n",
    "# 绘制柱状图\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, raw_attn, 0.4,  label='Raw Attention', color=colors[0])\n",
    "\n",
    "ax.bar(x+0.5, fast_attn, 0.4,  label='DiTFastAttn', color=colors[1])\n",
    "\n",
    "offset=0.5\n",
    "xs=np.concatenate([x,x+offset])\n",
    "others=np.concatenate([raw_other,raw_other])\n",
    "bar=ax.bar(xs, others, 0.4,  label='Others', color=colors[2])\n",
    "\n",
    "\n",
    "# ax.bar(x+0.2, raw_other, 0.4,  label='Others', color=bar[0].get_facecolor())\n",
    "\n",
    "# 设置 x 轴标签\n",
    "ax.set_xticks(x+offset/2)\n",
    "ax.set_xticklabels(labels)\n",
    "# ax.set_xlabel('Category')\n",
    "ax.set_ylabel('FLOPs Fraction')\n",
    "ax.set_title('Fraction of FLOPs for different resolutions')\n",
    "ax.legend(loc=3)\n",
    "\n",
    "plt.savefig(\"plot/flops.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "dir_raw=\"output/PixArt/PixArt-alpha_PixArt-Sigma-XL-2-1024-MS_steps20\"\n",
    "dir_fast=\"output/PixArt-alpha_PixArt-Sigma-XL-2-1024-MS_calib4_steps20_threshold0.95_window256_seqFalse\"\n",
    "for filename in os.listdir(dir_raw)[:30]:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        print(filename)\n",
    "        file_raw=os.path.join(dir_raw,filename)\n",
    "        file_fast=os.path.join(dir_fast,filename)\n",
    "        # concat vertically and show\n",
    "        img_raw=Image.open(file_raw)\n",
    "        img_fast=Image.open(file_fast)\n",
    "        img_concat=Image.new('RGB', (img_raw.width, img_raw.height + img_fast.height))\n",
    "        img_concat.paste(img_raw, (0, 0))\n",
    "        img_concat.paste(img_fast, (0, img_raw.height))\n",
    "        # img_concat=Image.new('RGB', (img_raw.width + img_fast.width, img_raw.height))\n",
    "        # img_concat.paste(img_raw, (0, 0))\n",
    "        # img_concat.paste(img_fast, (img_raw.width, 0))\n",
    "        # resize to 30%\n",
    "        img_concat=img_concat.resize((int(img_concat.width*0.25),int(img_concat.height*0.25)))\n",
    "        img_concat.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dit_fa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
