{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import dit_fast_attention\n",
    "import os\n",
    "file_path=os.path.abspath(dit_fast_attention.__file__)\n",
    "dir_path=os.path.dirname(file_path)\n",
    "os.chdir(dir_path)\n",
    "import matplotlib\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[\"full_attn+cfg_attn_share\",\"residual_window_attn\",\"residual_window_attn+cfg_attn_share\",\"output_share\"][::-1]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import calculate_fvd\n",
    "from tmp.common_metrics_on_video_quality.calculate_psnr import calculate_psnr\n",
    "from tmp.common_metrics_on_video_quality.calculate_ssim import calculate_ssim\n",
    "from tmp.common_metrics_on_video_quality.calculate_lpips import calculate_lpips\n",
    "NUMBER_OF_VIDEOS = 8\n",
    "VIDEO_LENGTH = 30\n",
    "CHANNEL = 3\n",
    "SIZE = 64\n",
    "# videos1 = torch.zeros(NUMBER_OF_VIDEOS, VIDEO_LENGTH, CHANNEL, SIZE, SIZE, requires_grad=False)\n",
    "# videos2 = torch.ones(NUMBER_OF_VIDEOS, VIDEO_LENGTH, CHANNEL, SIZE, SIZE, requires_grad=False)\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "def load_to_videos(file_path):\n",
    "    all_videos=[]\n",
    "    for file_name in os.listdir(file_path):\n",
    "        if file_name.endswith('.mp4'):\n",
    "            print(file_name)\n",
    "            video = cv2.VideoCapture(file_path+file_name)\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = video.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            video.release()\n",
    "            # BGR to RGB\n",
    "            frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames]\n",
    "            frames = np.array(frames)\n",
    "            frames = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2)/255\n",
    "            frames = frames.unsqueeze(0)\n",
    "            all_videos.append(frames)\n",
    "    all_videos = torch.cat(all_videos, dim=0)\n",
    "    return all_videos\n",
    "            \n",
    "videos1 = load_to_videos('output/opensora/_50_100_1.0_32_(240, 426)/')\n",
    "\n",
    "for threshold in [0.975,0.95,0.925,0.9,0.875,0.85]:\n",
    "    videos2=load_to_videos(f'output/opensora/_50_100_{threshold}_32_(240, 426)/')\n",
    "    # rst=calculate_fvd(raw_videos1, videos2, device, method='styleganv')\n",
    "    result={}\n",
    "    # result['ssim'] = calculate_ssim(videos1, videos2)\n",
    "    # result['psnr'] = calculate_psnr(videos1, videos2)\n",
    "    result['lpips'] = calculate_lpips(videos1, videos2, device)\n",
    "    print(result)\n",
    "    print(result['lpips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def extract_and_stitch_frames(video_files, frame_indexes, output_file):\n",
    "   \"\"\"\n",
    "   从视频文件中提取指定帧，并将它们横向拼接在一起。\n",
    "   多个视频文件的结果按照纵向拼接。\n",
    "   \n",
    "   :param video_files: 视频文件的列表。\n",
    "   :param frame_indexes: 要提取的帧的索引。\n",
    "   :param output_file: 输出拼接后的图像文件路径。\n",
    "   \"\"\"\n",
    "   all_frames = []\n",
    "   \n",
    "   for video_file in video_files:\n",
    "       cap = cv2.VideoCapture(video_file)\n",
    "       frames = []\n",
    "       \n",
    "       for idx in frame_indexes:\n",
    "           cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "           ret, frame = cap.read()\n",
    "           if ret:\n",
    "               frames.append(frame)\n",
    "           else:\n",
    "               print(f\"无法从 {video_file} 中读取第 {idx} 帧\")\n",
    "       \n",
    "       cap.release()\n",
    "       \n",
    "       # 将同一视频文件的所有帧横向拼接\n",
    "       stitched_frame = cv2.hconcat(frames)\n",
    "       all_frames.append(stitched_frame)\n",
    "   \n",
    "   # 将所有视频文件的拼接后的帧纵向拼接\n",
    "   stitched_image = cv2.hconcat(all_frames)\n",
    "   \n",
    "   cv2.imwrite(output_file, stitched_image)\n",
    "   print(f\"已将拼接后的图像保存到 {output_file}\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "for threshold in [1.0,0.975,0.95,0.925,0.9,0.875,0.85]:\n",
    "    video_files=[]\n",
    "    frames=[0,7,13]\n",
    "    for samplei in [2,4,6]:\n",
    "        video_file=f'output/opensora/_50_100_{threshold}_32_(240, 426)/_{samplei}.mp4'\n",
    "        video_files.append(video_file)\n",
    "    output_file = f'plot/stitched_frame_{threshold}.jpg'\n",
    "\n",
    "    extract_and_stitch_frames(video_files, frames, output_file)\n",
    "\n",
    "def add_caption_and_stack_images(images, output_file, font=cv2.FONT_HERSHEY_SIMPLEX):\n",
    "    \n",
    "    # 读取第一张图片，用于获取字体大小\n",
    "    image = cv2.imread(images[0])\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # 计算标题字体大小\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "    text = os.path.splitext(os.path.basename(images[0]))[0]  # 获取文件名作为标题\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    \n",
    "    # 确保图片宽度足够容纳标题\n",
    "    \n",
    "    if width < text_width + 10:\n",
    "        new_width = text_width + 10\n",
    "        new_height = int(height * (new_width / width))  # 保持图片的长宽比\n",
    "        image = cv2.resize(image, (new_width, new_height))\n",
    "    else:\n",
    "        new_width = width\n",
    "        new_height = height\n",
    "    \n",
    "    # 为图片添加标题\n",
    "    captioned_image = image.copy()\n",
    "    cv2.putText(captioned_image, text, (10, int(0.9 * new_height)), font, font_scale, (0, 0, 0), font_thickness)\n",
    "    \n",
    "    # 纵向拼接图片\n",
    "    total_images = len(images)\n",
    "    stacked_images = []\n",
    "    for img_path in images:\n",
    "        img = cv2.imread(img_path)\n",
    "        height, width, _ = img.shape\n",
    "        # 调整图片大小以匹配第一张图片的宽度\n",
    "        resized_img = cv2.resize(img, (new_width, int(height * (new_width / width))))\n",
    "        # 添加标题\n",
    "        captioned_img = resized_img.copy()\n",
    "        cv2.putText(captioned_img, os.path.splitext(os.path.basename(img_path))[0], (10, int(0.9 * new_height)), font, font_scale, (0, 0, 0), font_thickness)\n",
    "        stacked_images.append(captioned_img)\n",
    "    \n",
    "    # 计算纵向拼接后的图片高度\n",
    "    total_height = sum([img.shape[0] for img in stacked_images])\n",
    "    stacked_image = np.zeros((total_height, new_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # 拼接图片\n",
    "    y_offset = 0\n",
    "    for img in stacked_images:\n",
    "        stacked_image[y_offset:y_offset + img.shape[0], :, :] = img\n",
    "        y_offset += img.shape[0]\n",
    "    \n",
    "    # 保存拼接后的图片\n",
    "    # stacked_image=cv2.resize(stacked_image, fx=0.5,fy=0.5)\n",
    "    cv2.imwrite(output_file, stacked_image)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "image_folder = []\n",
    "for threshold in [1.0,0.95,0.9]:\n",
    "    image_folder.append(f'plot/stitched_frame_{threshold}.jpg')\n",
    "output_file = 'plot/stacked_images.jpg'  # 输出图片路径\n",
    "add_caption_and_stack_images(image_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from collections import namedtuple\n",
    "import re\n",
    "def parse_txt_data(txt_data):\n",
    "    data = []\n",
    "    for block in txt_data.split('\\n\\n'):\n",
    "        lines = block.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('Config '):\n",
    "                idx = line.find(\"{\")\n",
    "                namespace_dict=eval(line[idx:])\n",
    "                data.append(namespace_dict)\n",
    "            # elif line.startswith('calib_ssim'):\n",
    "            #     calib_ssim = float(line.split('=')[1])\n",
    "            #     # data.append(calib_ssim)\n",
    "            #     namespace_dict[\"calib_ssim\"] = calib_ssim\n",
    "            elif line.startswith('{'):\n",
    "                metrics = eval(line)\n",
    "                # data.append(metrics)\n",
    "                namespace_dict[\"metrics\"] = metrics\n",
    "            elif line.startswith('macs'):\n",
    "                macs = eval(line.split('=')[1])\n",
    "                namespace_dict[\"macs\"] = macs\n",
    "                # data.append(macs)\n",
    "            elif line.startswith('attn_mac'):\n",
    "                attn_mac = float(line.split('=')[1])\n",
    "                # data.append(attn_mac)\n",
    "                namespace_dict[\"attn_mac\"] = attn_mac\n",
    "            elif line.startswith('latencies'):\n",
    "                latencies = eval(line.split('=')[1])\n",
    "                # data.append(latencies)\n",
    "                namespace_dict[\"latencies\"] = latencies\n",
    "                # print(latencies)\n",
    "    return data\n",
    "\n",
    "txt_data=open(\"output/opensora_results.txt\").read()\n",
    "data=parse_txt_data(txt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation plot\n",
    "seqs=[1,0.975,0.95,0.925,0.9,0.875,0.85]\n",
    "# sfig, saxes = plt.subplots(1,3,figsize=(3.8*3,1.5))\n",
    "fig=plt.figure(figsize=(3,2))\n",
    "ax=fig.add_subplot(111)\n",
    "# titles=[\"512x512 DiT-XL\",\"1024x1024 PixArt-Sigma-XL\",\"2048x2048 PixArt-Sigma-XL\"]\n",
    "\n",
    "visited=[]\n",
    "attn_macs=[]\n",
    "for threshold in seqs:\n",
    "    for l in data[::-1]:\n",
    "        # print(l)\n",
    "        if l[\"threshold\"]==threshold and l[\"scheduler\"][\"num_sampling_steps\"]==50 and l[\"image_size\"]==(240,426):\n",
    "            if threshold in visited:\n",
    "                continue\n",
    "            visited.append(l[\"threshold\"])\n",
    "            attn_macs.append(l[\"attn_mac\"]/1e3)\n",
    "            # lats_all.append(l[\"latencies\"][f\"{nbatch}_all\"])\n",
    "            # lats_attn.append(l[\"latencies\"][f\"{nbatch}_attn\"])\n",
    "print(attn_macs)\n",
    "attn_macs=np.array(attn_macs)\n",
    "ax.plot(seqs,attn_macs,marker='o',label=\"Attention MACs (G)\")\n",
    "# print(attn_macs/attn_macs[0]*100)\n",
    "print_macs_frac=[f\"{x:.2f}\\%\" for x in 100-attn_macs/attn_macs[0]*100]\n",
    "print(', '.join(print_macs_frac))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def extract_and_stitch_frames(video_filess, frame_indexes, output_file):\n",
    "    \"\"\"\n",
    "    从视频文件中提取指定帧，并将它们横向拼接在一起。\n",
    "    多个视频文件的结果按照纵向拼接。\n",
    "\n",
    "    :param video_files: 视频文件的列表。\n",
    "    :param frame_indexes: 要提取的帧的索引。\n",
    "    :param output_file: 输出拼接后的图像文件路径。\n",
    "    \"\"\"\n",
    "    stitched_images=[]\n",
    "    for video_files in video_filess:\n",
    "        all_frames = []\n",
    "\n",
    "        for video_file in video_files:\n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            frames = []\n",
    "            \n",
    "            for idx in frame_indexes:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frames.append(frame)\n",
    "                else:\n",
    "                    print(f\"无法从 {video_file} 中读取第 {idx} 帧\")\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # 将同一视频文件的所有帧横向拼接\n",
    "            stitched_frame = cv2.vconcat(frames)\n",
    "            all_frames.append(stitched_frame)\n",
    "\n",
    "        # 将所有视频文件的拼接后的帧纵向拼接\n",
    "        stitched_image = cv2.vconcat(all_frames)\n",
    "        stitched_images.append(stitched_image)\n",
    "    # all_images=np.array(stitched_images)\n",
    "    all_images = cv2.hconcat(stitched_images)\n",
    "    \n",
    "    all_images=cv2.resize(all_images, None, fx=0.5, fy=0.5)\n",
    "    cv2.imwrite(output_file, all_images)\n",
    "    print(f\"已将拼接后的图像保存到 {output_file}\")\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "video_filess=[]\n",
    "for threshold in [1.0,0.975,0.95,0.925,0.9,0.875,0.85]:\n",
    "    video_files=[]\n",
    "    frames=[_ for _ in range(0,16,8)]\n",
    "    for samplei in range(0,10):\n",
    "        video_file=f'output/opensora/_50_100_{threshold}_32_(240, 426)/_{samplei}.mp4'\n",
    "        video_files.append(video_file)\n",
    "    \n",
    "    video_filess.append(video_files)\n",
    "output_file = f'plot/stitched_frame_appendix.jpg'\n",
    "extract_and_stitch_frames(video_filess, frames, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dit_fa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
